{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Streamlytics: A Data Science Approach to Netflix Content**    \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K8O9gbMmKGm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streamlytics – A Data Science Approach to Netflix Content\n",
        "In today's digital-first entertainment landscape, streaming platforms like Netflix have revolutionized content consumption. With a vast and diverse global audience, understanding what drives viewership, the nature of the content library, and evolving trends has become a key strategic need. “Streamlytics” is a Python-based data science project focused on extracting actionable insights from Netflix’s content catalog using exploratory data analysis (EDA), text processing, and machine learning techniques.\n",
        "\n",
        "The dataset used in this project includes metadata for thousands of Netflix titles such as title, type (Movie/TV Show), director, cast, country, release year, rating, duration, listed_in (genre), and description. The objective is to analyze this structured and unstructured data to uncover hidden trends, identify clusters of similar content, and potentially build classification models to predict the type of content based on metadata.\n",
        "\n",
        "**Project Objective**\n",
        "The primary objective of this project is to explore the Netflix dataset comprehensively and build meaningful data products and models that can support decision-making. The goal is not just limited to understanding trends but also includes building a classification model, performing clustering, and presenting insights that stakeholders (like data analysts, marketing teams, or content strategists) can act upon.\n",
        "\n",
        "**Scope of Work Based on Project Evaluation Criteria**\n",
        "**Understanding the Dataset and Problem Statement**\n",
        "The project begins with a clear identification of what needs to be achieved — understanding content patterns, predicting content types, identifying content clusters, and generating insights that reflect Netflix’s global content strategy.\n",
        "\n",
        "**Efficient EDA**\n",
        "The data is analyzed using Pandas, Seaborn, and Matplotlib to generate insights on:\n",
        "\n",
        "Growth in content over time\n",
        "\n",
        "Country-wise contributions\n",
        "\n",
        "Most frequent genres\n",
        "\n",
        "Duration and rating distributions\n",
        "\n",
        "Director and actor frequency\n",
        "\n",
        "**Dealing with Missing Values and Outliers**\n",
        "The dataset contains missing entries in columns like cast, director, and country. These are either dropped or filled strategically depending on the analysis needs. Outliers (e.g., extremely long durations or missing years) are identified and handled.\n",
        "\n",
        "**Cleaning the Document**\n",
        "The description field is cleaned using NLP techniques — removing punctuation, stopwords, and special characters to prepare it for vectorization.\n",
        "\n",
        "**Exploring Exceptional Cases**\n",
        "Titles with missing critical values, ambiguous types, or inconsistent ratings are explored as special cases to understand anomalies and how they affect the broader dataset.\n",
        "\n",
        "**Preprocessing – TFIDF / Bag of Words**\n",
        "For text-based modeling, TF-IDF and Bag of Words are used to transform the description column into usable numerical features. These are later used for clustering and classification.\n",
        "\n",
        "**Selecting the Approach and Algorithm**\n",
        "Based on the problem, both unsupervised and supervised learning methods are considered:\n",
        "\n",
        "Clustering: K-Means\n",
        "\n",
        "Classification: Logistic Regression, Random Forest\n",
        "\n",
        "**Modeling Using at Least 2 Algorithms**\n",
        "Models are trained and evaluated to classify content type and to cluster similar shows and movies.\n",
        "\n",
        "**Brief Strategy for Clusters Formed**\n",
        "Clustering aims to identify content buckets — such as family comedies, crime dramas, international thrillers — based on genre and textual patterns."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "https://github.com/shibamdutta99"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the explosive growth of digital streaming platforms, Netflix has become a global leader in on-demand video content. The company hosts thousands of titles across various genres, regions, and languages. However, as the content library expands, identifying patterns in viewership, content trends, and global distribution becomes increasingly complex.\n",
        "\n",
        "The primary challenge is to explore and analyze Netflix's content metadata to extract meaningful insights that can support business decision-making and personalized recommendations. This involves working with structured and unstructured data fields (like title, genre, country, cast, and description), understanding data distribution, and identifying trends over time. Additionally, there's a need to build classification and clustering models to:\n",
        "\n",
        "Predict the type of content (Movie or TV Show) based on metadata\n",
        "\n",
        "Group similar titles together based on textual and categorical features\n",
        "\n",
        "The objective is to transform raw Netflix content data into actionable insights and predictive models that can benefit stakeholders such as content strategists, data analysts, marketing teams, and product managers.\n",
        "\n",
        "**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Labmentix/Project 1 - Netflix/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 7,787 rows and 12 columns.\n",
        "\n",
        "It includes information such as title, type (Movie or TV Show), director, cast, country, date_added, release_year, duration, rating, listed_in (genre), and description.\n",
        "\n",
        "Several columns had missing data:\n",
        "\n",
        "director → 2,389 missing (~30.7%)\n",
        "\n",
        "cast → 718 missing (~9.2%)\n",
        "\n",
        "country → 507 missing (~6.5%)\n",
        "\n",
        "date_added → 10 missing (~0.1%)\n",
        "\n",
        "rating → 7 missing (~0.09%)\n",
        "\n",
        "Columns like title, type, release_year, duration, listed_in, and description were complete (no missing values).\n",
        "\n",
        "The dataset had no fully duplicated rows.\n",
        "\n",
        "The date_added column was stored as a string (object) instead of datetime format.\n",
        "\n",
        "The duration column combined numeric values with text (e.g., \"90 min\", \"2 Seasons\"), making it unsuitable for direct numeric operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the Netflix content was added between 2018 and 2021.\n",
        "\n",
        "A significant portion of content was originally released post-2013, suggesting Netflix has increasingly focused on modern content.\n",
        "\n",
        "The earliest title was released in 1925, showing that classic content is also part of the library, though rare.\n",
        "\n",
        "The month_added distribution shows Netflix updates content consistently across months, with slightly higher activity around April to October.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting date added column to datetime format\n",
        "\n",
        "df['date_added'] = pd.to_datetime(df['date_added'],errors='coerce')\n",
        "\n",
        "# Extract 'year_added' and 'month_added' from 'date_added'\n",
        "df['year_added']= df['date_added'].dt.year\n",
        "df['month_added']=df['date_added'].dt.month\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 'duration' into numerical and categorical (unit) values\n",
        "\n",
        "df[['duration_num', 'duration_type']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "n-rn5aHwVQ8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structured Duration:\n",
        "With duration_num and duration_type, we can now easily filter between movies and TV shows and analyze average durations.\n",
        "\n",
        "Ready-to-Use Dates:\n",
        "Converting date_added allows time-based analysis like trends in content addition per year or month.\n",
        "\n",
        "Categorical Cleanliness:\n",
        "By replacing missing categorical values with 'Unknown', we ensure no rows are lost while maintaining interpretability.\n",
        "\n",
        "Missing Data Insight:\n",
        "Some features like director and cast had high missing rates, suggesting not all titles are fully credited.\n",
        "\n",
        "Data Quality Improved:\n",
        "After dropping duplicates and handling nulls, the dataset is now reliable and ready for analysis/visualizations.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"charts\", exist_ok=True)\n",
        "import os\n",
        "print(\"Current Working Directory:\", os.getcwd())\n"
      ],
      "metadata": {
        "id": "cZGfQumxFLJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Countplot for Content distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='type', data=df, palette='viridis')\n",
        "plt.title('Distribution of Content Types')\n",
        "plt.xlabel('Content Type')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/distribution_of_content_types.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the how much content is 'Movie' vs 'TV Show'"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we can understand that Netflix is focusing more on Movies than TV Shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insight can help creating a postive business impact.\n",
        "Company can tailor marketing campaign for the movie enthusiasts.\n",
        "The large movie library could be a differentiator against competitors who may focus more heavily on TV series.\n",
        "The company can analyze which movie genres are most popular and further invest in those.\n",
        "\n",
        "\n",
        "There are insights that could lead to negative growth.\n",
        "The most critical insight is the significant imbalance in the content library.\n",
        "The small number of TV shows could lead to a high churn rate.\n",
        "Failure to Attract a Key Market Segment."
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2. Top 10 countries with most content - Bar Chart\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='country', data=df, order=df['country'].value_counts().index[:10], palette='viridis')\n",
        "plt.title('Top 10 Countries with Most Content')\n",
        "\n",
        "plt.savefig(\"charts/Top_10_Countries_with_Most_Content.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the content count across the top 10 countries."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States is the dominant source of content.\n",
        "\n",
        "There's a significant drop-off after the top two countries."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The insights can lead to positive growth by informing content strategy and market expansion.\n",
        "Explore expanding into countries like the UK and Japan, which are already showing a respectable amount of content.\n",
        "\n",
        "\n",
        "Countries like the United Kingdom, Japan, and Canada are mature digital markets. If content growth there is flat, it may reflect saturation.\n",
        "The United States and India dominate content volume, suggesting a high dependency on these markets.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3. Ratings Distribution - Bar Chart\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x = 'rating' , data = df, palette = 'viridis', order = df['rating'].value_counts().index)\n",
        "plt.title('Ratings Distribution')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/Ratings_Distribution.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because it's ideal for comparing the count of items across different, discrete categories"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratings TV-MA and TV-14 are by far the most frequent, with counts of around 2,900 and 1,900, respectively.\n",
        "\n",
        "Ratings for younger audiences, such as TV-Y, TV-G, and G, are significantly less common."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights can lead to positive business impact by informing content acquisition and marketing strategies.\n",
        "\n",
        "Marketing campaigns can be tailored to highlight the vast library of mature dramas, thrillers, and comedies.\n",
        "\n",
        "\n",
        "An insight that could lead to negative growth is the platform's severe lack of content for children and family audiences."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4. Release Year Trend - Line chart\n",
        "\n",
        "release_year_counts = df['release_year'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=release_year_counts.index, y = release_year_counts.values)\n",
        "plt.title('Release Year Trend')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.savefig(\"charts/Release_Year_Trend.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was the ideal choice to display the release year trend because it effectively shows how the number of content releases has changed over a continuous period of time"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a dramatic increase in content releases starting around the year 2010.\n",
        "There is a sharp and sudden drop-off in content releases after the 2019 peak, with the number of new titles falling significantly in 2020 and beyond."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "The data shows a period of rapid content acquisition, which can be leveraged for marketing.\n",
        "Acknowledging the peak in 2019 can help a company analyze what made that year so successful and potentially replicate the strategy.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "The sudden drop-off in new content could signal a stagnating or declining platform."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Top 10 Genres- Bar chart\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='listed_in', data = df, order = df['listed_in'].value_counts().index[:10], palette = 'viridis')\n",
        "plt.title('Genre Distribution')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.savefig(\"charts/Genre_Distribution.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vertical bar chart was chosen because it is the most effective way to compare the count of content across various, discrete genres"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common genres are Documentaries, Stand-Up Comedy.\n",
        "Lack of Family Content.\n",
        "High Volume of International Content."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "The platform can leverage its strengths by marketing itself as the premier destination for documentaries, stand-up comedy.\n",
        "The company can also use the data to acquire more popular international content and promote it to its existing international audience base.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "The low count of content in genres like Kids' TV and Children & Family Movies suggests that the platform is not effectively competing for the family market.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Content Type vs Release Year\n",
        "\n",
        "df_trends = df.groupby(['release_year','type']).size().reset_index(name = 'count')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot( x = 'release_year' , y = 'count', hue = 'type', data = df_trends)\n",
        "plt.title('Content Type vs Release Year')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/Content_Type_vs_Release_Year.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line chart with multiple lines to visualize the relationship between \"Content Type\" and \"Release Year\" because it is the most effective way to compare two separate trends over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Movies consistently outnumber TV shows in terms of content releases for almost the entire history of the platform.\n",
        " Both movies and TV shows show a dramatic increase in new releases, starting around 2010.\n",
        " Both content types peaked in new releases around 2019.\n",
        " There's a steep drop-off in new releases for both movies and TV shows after 2019."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "The insights can lead to a positive business impact by helping to inform content strategy and budget allocation.\n",
        "\n",
        "The company can leverage its strength in movies to attract subscribers.\n",
        "\n",
        "By analyzing the factors that led to the successful peak in 2019, the company could potentially replicate the strategy to revitalize its content pipeline, leading to renewed subscriber growth.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "An insight that could lead to negative growth is the sharp decline in new releases for both movies and TV shows after 2019.\n",
        "\n",
        "For most streaming services, a continuous influx of new content is essential for subscriber retention."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rating vs Content Type\n",
        "\n",
        "rating_by_type_counts = df.groupby(['rating','type']).size().reset_index(name = 'count')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x = 'rating', y = 'count' , hue = 'type', data = rating_by_type_counts)\n",
        "plt.title('Rating vs Content Type')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/Rating_vs_Content_Type.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a grouped bar chart to visualize the relationship between \"Rating\" and \"Content Type\" because it is the most effective way to directly compare the count of movies and TV shows for each individual rating category."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies generally outnumber TV shows across almost all rating categories.\n",
        "\n",
        "TV-MA is the most common rating for both movies (over 1,750 titles) and TV shows.\n",
        "\n",
        "While movies outnumber TV shows in most categories, TV shows have a slightly higher count in the TV-Y rating."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "The platform can leverage its strength in mature content (especially TV-MA and TV-14) to market itself as a primary destination for adult audiences.\n",
        "The insight about the TV-Y rating can be used to specifically target families.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "An insight that could lead to negative growth is the significant imbalance in the rating distribution."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import name\n",
        "# Country vs Content Type (Top 5 Countries)\n",
        "\n",
        "top_5_countries = df['country'].value_counts().index[:5]\n",
        "\n",
        "country_content_counts = df[df['country'].isin(top_5_countries)].groupby(['country','type']).size().reset_index(name = 'count')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x = 'country', y = 'count', data = country_content_counts, hue = 'type', order = top_5_countries)\n",
        "plt.title('Country vs Content Type (Top 5 Countries)')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/Country_vs_Content_Type.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose a grouped bar chart to visualize the relationship between \"Country\" and \"Content Type\" because it is the most effective way to directly compare the count of movies and TV shows for each of the top 5 countries."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States has a significantly higher number of movies and TV shows than any other country.\n",
        "Movies are the dominant content type in both the United States and India.\n",
        "The United Kingdom has a relatively balanced distribution, with a nearly equal number of movies and TV shows."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "\n",
        "\n",
        "The data clearly shows that the platform has a strong library of US and Indian movies. The company can leverage this to attract and retain subscribers in these markets.\n",
        "\n",
        "The insight about the high volume of TV shows in Japan could inform a strategy to acquire more Japanese TV series to further capture that market.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "\n",
        "\n",
        "India and the United States show a heavy skew toward movies, with TV show production lagging far behind.\n",
        "\n",
        "Underproduction of TV shows may limit cross-border content success, reducing global growth and monetization opportunities.\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type vs added year\n",
        "\n",
        "type_added_year_counts = df.groupby(['year_added','type']).size().reset_index(name = 'count')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x ='year_added', y = 'count', hue = 'type', data = type_added_year_counts)\n",
        "plt.title('Type vs Added Year')\n",
        "plt.xlabel('Added Year')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.savefig(\"charts/Type_vs_Added_Year.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MfkKl9KLtNJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a grouped bar chart to visualize the relationship between \"Content Type\" and \"Added Year\" because it’s the most effective way to compare the yearly count of movies and TV shows added to the platform."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The number of both movies and TV shows added to the platform started to increase significantly around 2017.\n",
        "\n",
        " The peak year for adding new content was 2019, with a total of over 2,100 titles added.\n",
        "\n",
        " There was a steep and sudden drop-off in new content acquisition after 2020, with very few titles added in 2021."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "\n",
        "The data provides a clear benchmark from 2019 that can be studied to understand what successful content acquisition looks like, and potentially inform future content strategies to recapture that growth.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "\n",
        "The dramatic decrease in new titles added after 2020 could lead to an increase in subscriber churn."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Country vs. Rating Distribution\n",
        "\n",
        "top_5_countries = df['country'].value_counts().nlargest(5).index\n",
        "\n",
        "# Filter the DataFrame to include only the top 5 countries.\n",
        "df_top_countries = df[df['country'].isin(top_5_countries)]\n",
        "\n",
        "# Create a cross-tabulation (pivot table) to count content by country and rating.\n",
        "stacked_data = pd.crosstab(df_top_countries['country'], df_top_countries['rating'])\n",
        "\n",
        "# Plot the stacked bar chart.\n",
        "stacked_data.plot(kind='bar', stacked=True, figsize=(15, 8))\n",
        "\n",
        "plt.title('Content Rating Distribution in Top 5 Countries')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Rating', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"charts/Content_Rating_Distribution_in_Top_5_Countries.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UZub00_g20uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the stacked bar chart to visualize the content rating distribution across the top 5 countries because it's an excellent way to show both the total content count for each country and the breakdown of that content by rating."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States has by far the largest amount of content, and its library is highly diverse in ratings.\n",
        "\n",
        "The most common ratings in the United States and India are TV-MA and TV-14, indicating a strong focus on content for mature and older teen audiences.\n",
        "\n",
        "While the US has a wide range of ratings, India and Japan show a more concentrated distribution.\n",
        "\n",
        "The presence of an \"Unknown\" rating category in the data points to a potential data quality issue."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**\n",
        "The data shows that the US has a very high volume of content with a diverse rating mix, which can be a key selling point for attracting a broad audience there. The platform can leverage the specific rating preferences observed in other top countries to acquire more of that type of content to better serve and grow its subscriber base in those regions.\n",
        "\n",
        "**Negative Growth Insight**\n",
        "The chart shows a relatively low volume of family-friendly content with ratings like TV-Y and TV-G across all countries, especially when compared to the mature ratings. If the platform is not acquiring enough content for families and children, it risks losing a significant market segment to competitors who offer a more balanced and diverse content library."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Genre Trend Over Time\n",
        "\n",
        "\n",
        "# First, we need to handle the 'listed_in' column by splitting it\n",
        "df_genres = df.assign(listed_in=df['listed_in'].str.split(', ')).explode('listed_in')\n",
        "\n",
        "# Get the top 5 most frequent genres to keep the chart clean.\n",
        "top_5_genres = df_genres['listed_in'].value_counts().nlargest(5).index\n",
        "\n",
        "# Filter the DataFrame to only include these top 5 genres.\n",
        "df_top_genres = df_genres[df_genres['listed_in'].isin(top_5_genres)].copy()\n",
        "\n",
        "# Now, group by release year and genre to get the counts.\n",
        "genre_trend = df_top_genres.groupby(['release_year', 'listed_in']).size().reset_index(name='count')\n",
        "\n",
        "# Create the line chart.\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.lineplot(data=genre_trend, x='release_year', y='count', hue='listed_in', style='listed_in')\n",
        "\n",
        "plt.title('Content Release Trends for Top 5 Genres Over Time')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.legend(title='Genre')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"charts/Content_Release_Trends_for_Top_5_Genres_Over_Time.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vV_UHDd97uWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a multi-line chart because it is the most effective way to visualize how multiple categories (in this case, the top 5 genres) have changed over a continuous period of time (Release Year)."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explosive Growth in the Last Decade.\n",
        "\n",
        "Content releases for all top genres peaked around 2019.\n",
        "\n",
        "There was a steep and sudden drop in content releases across all genres after the 2019 peak."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can definitely help create a positive business impact. The data clearly shows which genres, such as Dramas and International Movies, were most successful during the period of high growth.\n",
        "\n",
        "An insight that could lead to negative growth is the sharp decline in content releases across all genres after 2019. The justification is that a continuous flow of new content is a primary driver of subscriber retention and a key selling point for a streaming service."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "df_numeric = df.select_dtypes(include=['int64', 'float64'])\n",
        "df_numeric\n",
        "\n",
        "correlation_matrix = df_numeric.corr()\n",
        "plt.figure(figsize = (12,10))\n",
        "sns.heatmap(correlation_matrix, annot = True,cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"charts/correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F0Z0pgrRlbsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the correlation heatmap because it is the most effective way to visualize the relationships between multiple numerical variables in a single, clear chart."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a weak positive correlation (0.10) between release_year and year_added.\n",
        "\n",
        "There is a weak negative correlation (-0.13) between year_added and month_added.\n",
        "\n",
        "There is virtually no correlation (-0.01) between release_year and month_added, which is expected."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "columns_for_plot = ['release_year', 'year_added', 'month_added', 'type']\n",
        "df_for_pairplot = df[columns_for_plot].copy()\n",
        "\n",
        "sns.pairplot(df_for_pairplot, hue='type')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the pair plot because it is a highly effective way to visualize the relationships between all numerical variables in a single, comprehensive chart."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most titles are added in the same year they were released, but there is also a significant amount of older content being added over time, especially for movies.\n",
        "\n",
        "The distributions on the diagonal plots show a clear concentration of content that was both released and added in recent years, particularly around 2019-2020.\n",
        "\n",
        "The scatter plots involving month_added show no clear correlation with either release_year or year_added."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"charts\"))\n",
        "# Compress the entire folder\n",
        "!zip -r charts.zip charts\n",
        "\n",
        "# Download the zip\n",
        "from google.colab import files\n",
        "files.download(\"charts.zip\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HiFeSu5WGe1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "The proportion of TV Shows and Movies released in the United States is the same.\n",
        "⇒ 𝑝₁ = 𝑝₂\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "The proportion of TV Shows and Movies released in the United States is not the same.\n",
        "⇒ 𝑝₁ ≠ 𝑝₂\n",
        "\n",
        "This is a two-tailed z-test for proportions."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Filter non-null country values\n",
        "df_country = df[df['country'].notnull()]\n",
        "\n",
        "# Total counts\n",
        "tv_total = df_country[df_country['type'] == 'TV Show'].shape[0]\n",
        "movie_total = df_country[df_country['type'] == 'Movie'].shape[0]\n",
        "\n",
        "# US counts\n",
        "tv_us = df_country[(df_country['type'] == 'TV Show') & (df_country['country'] == 'United States')].shape[0]\n",
        "movie_us = df_country[(df_country['type'] == 'Movie') & (df_country['country'] == 'United States')].shape[0]\n",
        "\n",
        "# Counts and sample sizes\n",
        "counts = [tv_us, movie_us]\n",
        "nobs = [tv_total, movie_total]\n",
        "\n",
        "# Two-proportion z-test\n",
        "stat, pval = proportions_ztest(count=counts, nobs=nobs)\n",
        "\n",
        "# Output results\n",
        "print(\"Z-statistic:\", stat)\n",
        "print(\"p-value:\", pval)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if pval < alpha:\n",
        "    print(\"❌ Reject Null Hypothesis: Proportion of US content differs between TV Shows and Movies.\")\n",
        "else:\n",
        "    print(\"✅ Fail to Reject Null Hypothesis: No significant difference in US content proportion.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-proportion Z-test\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Two-Proportion Z-test is appropriate when comparing proportions (percentages) between two groups or data is categorical, not continuous."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "The proportion of Movies = proportion of TV Shows on the platform.\n",
        "(There is no significant difference in counts.)\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "The proportion of Movies ≠ proportion of TV Shows.\n",
        "(There is a significant difference in the number of Movies and TV Shows.)\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Count the number of Movies and TV Shows\n",
        "movie_count = df[df['type'] == 'Movie'].shape[0]\n",
        "tvshow_count = df[df['type'] == 'TV Show'].shape[0]\n",
        "\n",
        "# Counts of successes in each group (we'll test against total)\n",
        "counts = [movie_count, tvshow_count]\n",
        "\n",
        "# Total observations in each group (equal to respective counts here)\n",
        "nobs = [movie_count + tvshow_count] * 2\n",
        "\n",
        "# Perform two-proportion z-test\n",
        "stat, pval = proportions_ztest(count=counts, nobs=nobs, alternative='two-sided')\n",
        "\n",
        "print(\"Z-statistic:\", stat)\n",
        "print(\"P-value:\", pval)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if pval < alpha:\n",
        "    print(\"❌ Reject Null Hypothesis: There is a significant difference in the number of Movies and TV Shows.\")\n",
        "else:\n",
        "    print(\"✅ Fail to Reject Null Hypothesis: There is no significant difference in the number of Movies and TV Shows.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-Proportion Z-Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the Two-Proportion Z-Test because:\n",
        "\n",
        "We are comparing two categorical groups and the goal is to test whether the proportions of these two categories are significantly different."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant correlation between release_year and year_added.\n",
        "\n",
        "Alternative Hypothesis (Hₐ):\n",
        "There is a significant positive correlation between release_year and year_added.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Ensure both columns are numeric and drop rows with NaNs\n",
        "df_filtered = df[['release_year', 'year_added']].dropna()\n",
        "\n",
        "# Pearson correlation\n",
        "corr_coeff, p_value = pearsonr(df_filtered['release_year'], df_filtered['year_added'])\n",
        "\n",
        "print(\"Pearson Correlation Coefficient:\", corr_coeff)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"❌ Reject Null Hypothesis: There is a significant positive correlation between release_year and year_added.\")\n",
        "else:\n",
        "    print(\"✅ Fail to Reject Null Hypothesis: There is no significant correlation between release_year and year_added.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Coefficient test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson Correlation Coefficient test is appropriate when:\n",
        "\n",
        "Both variables are continuous and numeric (e.g., release_year and year_added).\n",
        "\n",
        "You want to measure the linear relationship between two variables."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Fill missing 'director' and 'cast' with placeholder text\n",
        "df['director'].fillna('No Director Info', inplace=True)\n",
        "df['cast'].fillna('No Cast Info', inplace=True)\n",
        "\n",
        "# Fill missing 'country' with the mode\n",
        "df['country'].fillna(df['country'].mode()[0], inplace=True)\n",
        "\n",
        "# Fill missing 'rating' with forward fill\n",
        "df['rating'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Fill missing 'date_added' with the most frequent value (mode)\n",
        "df['date_added'].fillna(df['date_added'].mode()[0], inplace=True)\n",
        "df['month_added'].fillna(df['month_added'].mode()[0], inplace=True)\n",
        "df['year_added'].fillna(df['year_added'].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\nMissing Values After Imputation:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Placeholder Text Imputation\n",
        "\n",
        "Columns: Director, Cast\n",
        "\n",
        "Replaced missing values in columns with a placeholder string.\n",
        "\n",
        "Imputing with a placeholder avoids nulls while preserving the signal that data was missing.\n",
        "\n",
        "\n",
        "2. Mode Imputation\n",
        "\n",
        "Columns: country, date_added, month_added, year_added\n",
        "\n",
        "Technique: Filled missing values with the most frequently occurring value (mode).\n",
        "\n",
        "Mode Imputation is suitable for categorical or nominal data. Columns like Date_added are important for time-based analysis, so imputing with the most common date ensures consistency.\n",
        "\n",
        "3. Forward Fill (ffill) Imputation\n",
        "\n",
        "Column: rating\n",
        "\n",
        "Technique: Propagates the last valid observation forward.\n",
        "\n",
        "Why:\n",
        "\n",
        "Rating usually follows a trend in the dataset order.Since only 7 values were missing, forward fill maintains natural flow without distorting the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "def detect_outliers_iqr(df, column):\n",
        "    # Convert the column to numeric, coercing errors to NaN\n",
        "    # This conversion will be done outside the function now to ensure persistence\n",
        "    df_cleaned = df.dropna(subset=[column])\n",
        "\n",
        "    Q1 = df_cleaned[column].quantile(0.25)\n",
        "    Q3 = df_cleaned[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df_cleaned[(df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Redefine remove_outliers to use the stored bounds\n",
        "def remove_outliers(df, col, lower_bound, upper_bound):\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    return df\n",
        "\n",
        "# Ensure duration_num is numeric before outlier detection and removal\n",
        "df['duration_num'] = pd.to_numeric(df['duration_num'], errors='coerce')\n",
        "df.dropna(subset=['duration_num'], inplace=True) # Drop rows where conversion failed\n",
        "\n",
        "# Apply outlier detection and removal for 'duration_num'\n",
        "outliers_duration, low, high = detect_outliers_iqr(df, 'duration_num')\n",
        "print(f\"Outliers in 'duration_num': {outliers_duration.shape[0]}\")\n",
        "df = remove_outliers(df, 'duration_num', low, high)\n",
        "\n",
        "\n",
        "# Apply outlier detection and removal for 'release_year'\n",
        "df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce')\n",
        "df.dropna(subset=['release_year'], inplace=True)\n",
        "outliers_release_year, low, high = detect_outliers_iqr(df, 'release_year')\n",
        "print(f\"Outliers in 'release_year': {outliers_release_year.shape[0]}\")\n",
        "df = remove_outliers(df, 'release_year', low, high)\n",
        "\n",
        "\n",
        "# Apply outlier detection and removal for 'year_added'\n",
        "df['year_added'] = pd.to_numeric(df['year_added'], errors='coerce')\n",
        "df.dropna(subset=['year_added'], inplace=True)\n",
        "outliers_year_added, low, high = detect_outliers_iqr(df, 'year_added')\n",
        "print(f\"Outliers in 'year_added': {outliers_year_added.shape[0]}\")\n",
        "df = remove_outliers(df, 'year_added', low, high)\n",
        "\n",
        "print(\"\\nShape after outlier removal:\", df.shape)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the IQR method for outlier detection and removed the rows containing outliers. This is a standard and effective technique for handling outliers in non-normally distributed, numeric features during data cleaning before analysis or modeling."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder, MultiLabelBinarizer\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Binary Encoding: 'type'\n",
        "if 'type' in df.columns:\n",
        "    df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "else:\n",
        "    print(\"Warning: 'type' column not found. Skipping binary encoding for 'type'.\")\n",
        "\n",
        "# 2. Ordinal Encoding: 'rating'\n",
        "if 'rating' in df.columns:\n",
        "    rating_order = [\n",
        "        'NR', 'UR', 'TV-Y', 'TV-G', 'TV-Y7', 'TV-Y7-FV',\n",
        "        'G', 'PG', 'TV-PG', 'PG-13', 'TV-14', 'R', 'TV-MA', 'NC-17', 'Unknown'\n",
        "    ]\n",
        "    df['rating'] = df['rating'].fillna('NR')\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[rating_order])\n",
        "    df['rating_encoded'] = ordinal_encoder.fit_transform(df[['rating']])\n",
        "    df.drop('rating', axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Warning: 'rating' column not found. Skipping ordinal encoding for 'rating'.\")\n",
        "\n",
        "# 3. Frequency Encoding: 'country'\n",
        "if 'country' in df.columns:\n",
        "    df['country'] = df['country'].fillna('Unknown')\n",
        "    country_freq = df['country'].value_counts().to_dict()\n",
        "    df['country_encoded'] = df['country'].map(country_freq)\n",
        "    df.drop('country', axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Warning: 'country' column not found. Skipping frequency encoding for 'country'.\")\n",
        "\n",
        "# 4. Date Features: 'date_added'\n",
        "if 'date_added' in df.columns:\n",
        "    df['date_added'] = pd.to_datetime(df['date_added'])\n",
        "    df['year_added'] = df['date_added'].dt.year\n",
        "    df['month_added'] = df['date_added'].dt.month\n",
        "    df.drop('date_added', axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Warning: 'date_added' column not found. Skipping date feature extraction.\")\n",
        "\n",
        "# 5. MultiLabelBinarizer: 'listed_in'\n",
        "if 'listed_in' in df.columns:\n",
        "    df['listed_in'] = df['listed_in'].fillna('Unknown')\n",
        "    df['listed_in'] = df['listed_in'].apply(lambda x: [genre.strip() for genre in x.split(',')])\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    listed_in_encoded = pd.DataFrame(mlb.fit_transform(df['listed_in']), columns=mlb.classes_)\n",
        "    df = pd.concat([df, listed_in_encoded], axis=1)\n",
        "    df.drop('listed_in', axis=1, inplace=True)\n",
        "else:\n",
        "    print(\"Warning: 'listed_in' column not found. Skipping MultiLabelBinarizer encoding.\")\n",
        "\n",
        "# 6. Optional: Encode 'release_year' (e.g., binning into decades)\n",
        "# df['release_decade'] = (df['release_year'] // 10) * 10\n",
        "\n",
        "# ✅ Final Preview\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Binary Encoding → For 'type'\n",
        "Column: type (either \"Movie\" or \"TV Show\")\n",
        "\n",
        "Technique Used: Binary/Label Encoding (0 for Movie, 1 for TV Show)\n",
        "\n",
        "Why: This column has only two unique values (binary category), so a simple 0/1 mapping is sufficient and avoids unnecessary dimensionality.\n",
        "\n",
        "\n",
        "2. Ordinal Encoding → For 'rating'\n",
        "Column: rating (e.g., G, PG, PG-13, R, etc.)\n",
        "\n",
        "Technique Used: Ordinal Encoding\n",
        "\n",
        "Why: Ratings have a natural order from most child-safe to most adult-rated. Preserving this order is important for meaningful numerical representation.\n",
        "\n",
        "\n",
        "3. Frequency Encoding → For 'country'\n",
        "Column: country (many unique values)\n",
        "\n",
        "Technique Used: Frequency Encoding\n",
        "\n",
        "Why: There are many distinct countries. One-hot encoding would create too many columns. Frequency encoding captures the importance/popularity of each country without increasing dimensionality.\n",
        "\n",
        "4. Multi-Label Binarization (Multi-hot Encoding) → For 'listed_in'\n",
        "Column: listed_in (comma-separated genres like \"Dramas, Crime, Action\")\n",
        "\n",
        "Technique Used: Multi-label Binarization using MultiLabelBinarizer\n",
        "\n",
        "Why: Each row can belong to multiple genres, so we treat it as a multi-label classification and apply one-hot encoding for each unique genre.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "# Step 4: Expand contractions in the 'description' column\n",
        "df['description'] = df['description'].apply(lambda x: contractions.fix(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 5: Display a few expanded descriptions\n",
        "df[['description']].head()\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Convert all text in the 'description' column to lowercase\n",
        "df['description'] = df['description'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "# Optional: view results\n",
        "df[['description']].head()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Step 3: Remove punctuation from the 'description' column\n",
        "df['description'] = df['description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def clean_text(text):\n",
        "    # Check if the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return text # Return non-string values as they are\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove words containing digits\n",
        "    text = ' '.join([word for word in text.split() if not any(char.isdigit() for char in word)])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the description column\n",
        "df['description'] = df['description'].apply(clean_text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stopwords set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from 'description'\n",
        "df['description'] = df['description'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]) if isinstance(x, str) else x\n",
        ")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "# Remove extra white spaces\n",
        "df['description'] = df['description'].apply(\n",
        "    lambda x: ' '.join(x.split()) if isinstance(x, str) else x\n",
        ")\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def synonym_replace(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    words = word_tokenize(text)\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            syn_word = synonyms[0].lemmas()[0].name()\n",
        "            new_words.append(syn_word.replace('_', ' '))\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "df['description'] = df['description'].apply(synonym_replace)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Only needed once\n",
        "\n",
        "# Tokenize each description\n",
        "df['tokens'] = df['description'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use lemmatization, because it reduces words to their base or dictionary form (lemma) while preserving their actual meaning, which helps in building better text-based ML models."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "DD3rJxAuCOdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "R37UPSFECivc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Netflix uses machine learning to personalize movie recommendations.\"\n",
        "\n",
        "# Tokenize and tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the missing resource\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorization, Because:\n",
        "\n",
        "It Captures Importance of Words\n",
        "Improves Feature Representation for ML Models\n",
        "Reduces Noise\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def manipulate_numerical_features(df):\n",
        "    \"\"\"\n",
        "    Minimizes feature correlation and creates new features.\n",
        "    \"\"\"\n",
        "    df_manipulated = df.copy()\n",
        "\n",
        "    # ====================================================================\n",
        "    # 1. Minimize Feature Correlation\n",
        "    # ====================================================================\n",
        "    print(\"Minimizing feature correlation...\")\n",
        "\n",
        "    # Select only the numeric columns for correlation analysis\n",
        "    df_numeric = df_manipulated.select_dtypes(include=['int64', 'float64'])\n",
        "    corr_matrix = df_numeric.corr().abs()\n",
        "\n",
        "    # Identify the upper triangle of the correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find columns to drop based on a correlation threshold (e.g., 0.8)\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "\n",
        "    if to_drop:\n",
        "        df_manipulated.drop(columns=to_drop, inplace=True)\n",
        "        print(\"Columns dropped due to high correlation:\", to_drop)\n",
        "    else:\n",
        "        print(\"No columns were dropped based on the 0.8 correlation threshold.\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ====================================================================\n",
        "    # 2. Create New Features\n",
        "    # ====================================================================\n",
        "    print(\"Creating new features...\")\n",
        "\n",
        "    # Create the new feature 'age_when_added'\n",
        "    if 'release_year' in df_manipulated.columns and 'year_added' in df_manipulated.columns:\n",
        "        df_manipulated.loc[:, 'age_when_added'] = df_manipulated['year_added'] - df_manipulated['release_year']\n",
        "        print(\"New feature 'age_when_added' created.\")\n",
        "    else:\n",
        "        print(\"Warning: Could not create 'age_when_added'. 'release_year' or 'year_added' columns are missing.\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Feature manipulation is complete. The final DataFrame is 'df_manipulated'.\")\n",
        "    return df_manipulated"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = manipulate_numerical_features(df)\n",
        "\n",
        "# 2. Handle any potential NaN values that might have been introduced\n",
        "df_final = df_final.fillna(0)\n",
        "\n",
        "# 3. Define your target variable (y) and feature matrix (X)\n",
        "#    We assume the target is 'type_encoded'. If this column does not exist\n",
        "#    in your DataFrame, please add it from your categorical encoding step.\n",
        "if 'type_encoded' in df_final.columns:\n",
        "    y = df_final['type_encoded']\n",
        "    X = df_final.drop(columns=['type_encoded'])\n",
        "\n",
        "    print(\"Shape of X (features):\", X.shape)\n",
        "    print(\"Shape of y (target):\", y.shape)\n",
        "\n",
        "    # 4. Perform Feature Selection\n",
        "    print(\"\\nStarting feature selection...\")\n",
        "    feature_scores = mutual_info_classif(X, y, random_state=42)\n",
        "\n",
        "    feature_scores_df = pd.DataFrame(\n",
        "        {'Feature': X.columns, 'Score': feature_scores}\n",
        "    ).sort_values(by='Score', ascending=False)\n",
        "\n",
        "    print(\"\\nFeatures ranked by importance:\")\n",
        "    print(feature_scores_df.head(20))\n",
        "\n",
        "    top_features = feature_scores_df['Feature'].head(10).tolist()\n",
        "    print(\"\\nTop 10 features selected for the model:\", top_features)\n",
        "else:\n",
        "    print(\"Error: 'type_encoded' column not found for feature selection.\")\n",
        "    print(\"Please ensure your categorical encoding step has been completed.\")"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used a filter method for feature selection. Reason:\n",
        "\n",
        "Simplicity and Speed, Relevance to the Target, Model Agnostic."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoded Genres (from listed_in): The genre of a title is a very strong indicator of whether it's a movie or a TV show.\n",
        "\n",
        "One-Hot Encoded Ratings: The rating system is often separated for movies and TV shows.\n",
        "\n",
        "One-Hot Encoded Countries: The country of origin can influence the type of content produced.\n",
        "\n",
        "Duration_num: The duration of a title is a very direct and important predictor."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data has already been transformed previously, so no transformation is needed."
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    # numeric columns and any engineered features.\n",
        "    numeric_cols = [\n",
        "        'release_year',\n",
        "        'year_added',\n",
        "        'duration_num',\n",
        "        'age_when_added'\n",
        "    ]\n",
        "\n",
        "    # 2. Check if the columns exist before attempting to scale\n",
        "    cols_to_scale = [col for col in numeric_cols if col in X.columns]\n",
        "\n",
        "    if cols_to_scale:\n",
        "        scaler = StandardScaler()\n",
        "        X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
        "        print(\"Numerical features have been successfully scaled.\")\n",
        "        print(\"\\nSample of scaled data (first 5 rows):\")\n",
        "        print(X[cols_to_scale].head())\n",
        "    else:\n",
        "        print(\"No numerical columns found to scale.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: 'X' or 'df_final' is not defined. Please ensure your DataFrame is correctly loaded and prepared.\")\n",
        "except KeyError:\n",
        "    print(\"Error: One of the columns in 'numeric_cols' was not found in your DataFrame 'X'.\")"
      ],
      "metadata": {
        "id": "UbFq3phPM9xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is necessary, especially for the textual data. When you convert text into numerical features using methods like TF-IDF, you often create a huge number of features (a high-dimensional space), which can lead to:\n",
        "\n",
        "The Curse of Dimensionality: This makes it difficult for machine learning models to find patterns and increases the risk of overfitting.\n",
        "\n",
        "Computational Inefficiency: A large number of features increases the time and memory required to train a model.\n",
        "\n",
        "Irrelevant and Redundant Features: Many of the generated features might be noisy or highly correlated, hindering the model's performance."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dimensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Assume 'df' is your original DataFrame and 'description' is a cleaned text column.\n",
        "# If you have a different column for text, adjust the line below.\n",
        "# The `fillna` ensures no errors occur if there are any remaining nulls.\n",
        "df['description'] = df['description'].fillna('')\n",
        "\n",
        "# Instantiate and fit the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['description'])\n",
        "\n",
        "# Apply PCA for dimensionality reduction on the TF-IDF matrix\n",
        "# Reduce to 500 components as a good starting point for a large dataset\n",
        "n_components = 500\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "pca_components = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# Create a DataFrame with the PCA features\n",
        "pca_df = pd.DataFrame(pca_components, columns=[f'pca_{i+1}' for i in range(n_components)])\n",
        "\n",
        "# Assuming your scaled numerical features are in a DataFrame named `df_scaled_numeric`.\n",
        "# You will need to make sure the indices of all DataFrames are aligned before concatenating.\n",
        "# For example, by using `df_scaled_numeric.reset_index(drop=True)`\n",
        "# Then, concatenate the scaled numerical features with the PCA components.\n",
        "# Let's create a placeholder for this step:\n",
        "# df_final = pd.concat([df_scaled_numeric, pca_df], axis=1)\n",
        "\n",
        "print(f\"Original TF-IDF feature count: {tfidf_matrix.shape[1]}\")\n",
        "print(f\"Reduced PCA feature count: {pca_components.shape[1]}\")\n",
        "# print(f\"Final DataFrame shape after dimensionality reduction: {df_final.shape}\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use Principal Component Analysis (PCA).\n",
        "\n",
        "PCA is an excellent choice for this task because it is an unsupervised technique that is perfect for numerical, high-dimensional data like the output of a TF-IDF vectorizer."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "KtIHPbteRbfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df_final['type_TV Show']\n",
        "X = df_final.drop(columns=['type_TV Show'])\n",
        "\n",
        "# You may also need to drop other non-numeric or redundant columns\n",
        "# before splitting, such as 'show_id', 'title', 'director', 'cast',\n",
        "# 'duration', 'description', 'duration_type', and 'tokens'.\n",
        "# For example:\n",
        "columns_to_drop = [\n",
        "    'show_id', 'title', 'director', 'cast', 'duration', 'description',\n",
        "    'duration_type', 'tokens'\n",
        "]\n",
        "\n",
        "# Ensure the columns exist before dropping them\n",
        "columns_to_drop_existing = [col for col in columns_to_drop if col in X.columns]\n",
        "X = X.drop(columns=columns_to_drop_existing)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 70/30 split is a well-established and effective ratio for this project.\n",
        "\n",
        "70% for Training: This provides the machine learning model with a large enough sample of data to learn.\n",
        "\n",
        "30% for Testing: This ensures a sufficiently large, independent sample to evaluate the model's performance on unseen data.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, based on your EDA in Chart - 1, the dataset is imbalanced. The countplot for content types clearly showed a much higher number of movies than TV shows. This creates a class imbalance where the majority class (movies) is over-represented.\n",
        "\n",
        "This is a problem for machine learning because a model might learn to be biased towards the majority class"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Ensure the target variables are of a suitable type for SMOTE\n",
        "# Convert boolean/object types to integers 0 and 1\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "\n",
        "# Instantiate SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE only to the training data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Original training data class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nResampled training data class distribution:\")\n",
        "print(y_train_resampled.value_counts())"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE is an excellent technique for addressing class imbalance. Instead of simply duplicating existing data, it creates synthetic examples of the minority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation -- Logistic Regression\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Assuming X_train_resampled, y_train_resampled, X_test, and y_test are available.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Fit the Algorithm on the resampled data\n",
        "log_reg.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the model using the original (non-resampled) test data\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log_reg))\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['Accuracy', 'Precision (TV Show)', 'Recall (TV Show)', 'F1-Score (TV Show)']\n",
        "report = classification_report(y_test, y_pred_log_reg, output_dict=True)\n",
        "scores = [\n",
        "    accuracy_score(y_test, y_pred_log_reg),\n",
        "    report['1']['precision'],\n",
        "    report['1']['recall'],\n",
        "    report['1']['f1-score']\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=metrics, y=scores, palette='viridis')\n",
        "plt.title('Logistic Regression Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for visualization\n",
        "cm_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_log_reg, annot=True, fmt='d', cmap='Blues', xticklabels=['Movie', 'TV Show'], yticklabels=['Movie', 'TV Show'])\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# -- GridSearchCv\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the tuned model\n",
        "y_pred_tuned_log_reg = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"Tuned Logistic Regression Performance:\")\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_log_reg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned_log_reg))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use GridSearchCV for hyperparameter tuning. This method exhaustively searches through a specified parameter grid, testing every possible combination."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no improvement in performance after hyperparameter tuning of the Logistic Regression model."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the model\n",
        "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the Algorithm on the resampled data\n",
        "rf_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ur8FWOqCVJU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "report_rf = classification_report(y_test, y_pred_rf, output_dict=True)\n",
        "scores_rf = [\n",
        "    accuracy_score(y_test, y_pred_rf),\n",
        "    report_rf['1']['precision'],\n",
        "    report_rf['1']['recall'],\n",
        "    report_rf['1']['f1-score']\n",
        "]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=metrics, y=scores_rf, palette='viridis')\n",
        "plt.title('Random Forest Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Afr7NxZ8VSzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time. It operates by building many trees and then outputting the mode of the classes as the prediction.\n",
        "\n",
        "Summary of Performance\n",
        "Accuracy: 99.70% – The model correctly classified almost all instances.\n",
        "\n",
        "Precision:\n",
        "\n",
        "Class 0: 1.00 (no false positives)\n",
        "\n",
        "Class 1: 0.99 (very few false positives)\n",
        "\n",
        "Recall:\n",
        "\n",
        "Class 0: 1.00 (no false negatives)\n",
        "\n",
        "Class 1: 1.00 (no false negatives)\n",
        "\n",
        "F1-Score:\n",
        "\n",
        "Balanced and very high for both classes, indicating excellent performance."
      ],
      "metadata": {
        "id": "E1LbaR91VnMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a more focused parameter grid to avoid excessive run time\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search_rf = GridSearchCV(rf_clf, param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the tuned model\n",
        "y_pred_tuned_rf = grid_search_rf.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"Tuned Random Forest Performance:\")\n",
        "print(\"Best Hyperparameters:\", grid_search_rf.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned_rf))"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use GridSearchCV again to find the best hyperparameters for the Random Forest model. This systematic approach ensures the optimal combination of parameters like n_estimators, max_depth, and min_samples_leaf is identified, which is crucial for maximizing the model's predictive power."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a slight improvement in the performance of the Random Forest model after hyperparameter tuning.\n",
        "\n",
        "Tuned Random Forest shows a slight but meaningful improvement in accuracy and F1-score for the minority class."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics and Business Impact\n",
        "Accuracy (0.997): Accuracy represents the proportion of total predictions that were correct. An accuracy of nearly 100% means the model is exceptionally good at correctly classifying both movies and TV shows.\n",
        "\n",
        "Business Impact: This high accuracy is excellent for internal content management and reporting. It suggests that the model can be used reliably for tasks like automatically tagging new content, which reduces manual effort and improves data quality.\n",
        "\n",
        "Precision (Class 1 - TV Show: 0.99): Precision measures the proportion of positive predictions (in this case, a title being a TV show) that were actually correct. A precision of 0.99 for TV shows means that when the model predicts a title is a TV show, it is correct 99% of the time.\n",
        "\n",
        "Business Impact: High precision is critical for the user experience. If a user filters for \"TV shows,\" they expect to see only TV shows. A low precision would mean many movies would be incorrectly shown, leading to user frustration. This high score indicates the model is reliable for filtering and recommendation systems.\n",
        "\n",
        "Recall (Class 1 - TV Show: 1.00): Recall, also known as sensitivity, measures the proportion of all actual positive cases (all TV shows in the test set) that the model correctly identified. A recall of 1.00 means the model successfully identified every single TV show in the test set.\n",
        "\n",
        "Business Impact: High recall is essential for ensuring that no relevant content is missed. If a user is searching for a specific TV show, a low recall would mean the model might fail to show them the correct title. A perfect recall score suggests the model is highly effective at finding all available TV shows, which is great for search functionality and content discoverability.\n",
        "\n",
        "F1-Score (Class 1 - TV Show: 1.00): The F1-score is the harmonic mean of precision and recall. It's a useful metric for imbalanced datasets because it provides a single score that balances both metrics. An F1-score of 1.00 indicates a perfect balance of precision and recall.\n",
        "\n",
        "Business Impact: The F1-score confirms the model's robustness and reliability, especially for the minority class (TV shows). It shows that the model is not achieving high accuracy by simply ignoring the minority class; instead, it is performing exceptionally well across both classes. This makes the model a strong candidate for deployment in a production environment.\n",
        "\n",
        "Overall Business Impact of the ML Model\n",
        "The tuned Random Forest model is an excellent tool for the \"Streamlytics\" project. Its near-perfect performance suggests it can have a significant positive impact on business operations. The model can:\n",
        "\n",
        "Improve Content Tagging: Automate the classification of new content, saving time and resources.\n",
        "\n",
        "Enhance User Experience: Power accurate filters and search functions, leading to higher user satisfaction and engagement.\n",
        "\n",
        "Inform Content Strategy: The model's feature importance (which you can analyze separately) could reveal which metadata points are most predictive of content type, helping content strategists make more informed acquisition decisions.\n",
        "\n",
        "Reduce Churn: By providing a highly accurate and reliable content discovery experience, the platform can reduce user frustration and increase subscriber retention."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# -- Gradient Boosting Classifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# ML Model - 3 Implementation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Fit the Algorithm on the resampled training data\n",
        "gb_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the model using the original test data\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Gradient Boosting Classifier Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_gb))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['Accuracy', 'Precision (TV Show)', 'Recall (TV Show)', 'F1-Score (TV Show)']\n",
        "report = classification_report(y_test, y_pred_gb, output_dict=True)\n",
        "scores = [\n",
        "    accuracy_score(y_test, y_pred_gb),\n",
        "    report['1']['precision'],\n",
        "    report['1']['recall'],\n",
        "    report['1']['f1-score']\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=metrics, y=scores, palette='viridis')\n",
        "plt.title('Gradient Boosting Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for visualization\n",
        "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Blues', xticklabels=['Movie', 'TV Show'], yticklabels=['Movie', 'TV Show'])\n",
        "plt.title('Confusion Matrix for Gradient Boosting Classifier')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w9U7syzjXY-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# ML Model - 3 Implementation with hyperparameter optimization techniques\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search_gb = GridSearchCV(gb_clf, param_grid_gb, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the tuned model\n",
        "y_pred_tuned_gb = grid_search_gb.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"Tuned Gradient Boosting Classifier Performance:\")\n",
        "print(\"Best Hyperparameters:\", grid_search_gb.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tuned_gb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_tuned_gb))\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used GridSearchCV for hyperparameter tuning. This technique systematically works through multiple combinations of parameter values, cross-validating each combination to find the optimal set of hyperparameters that yields the best performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, tuning did not improve performance. In fact, accuracy slightly dropped from 0.99827 to 0.99784.\n",
        "\n",
        "All other metrics remained identical, indicating no benefit from the hyperparameter tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recall (Sensitivity / True Positive Rate)\n",
        "Measures how well the model captures actual positive cases (e.g., defaulters, churners, frauds).\n",
        "\n",
        "✅ Business Impact:\n",
        "Ensures critical cases are not missed, reducing risk or loss.\n",
        "\n",
        "2. Precision\n",
        "Measures how many of the model’s positive predictions were actually correct.\n",
        "\n",
        "High precision = fewer false alarms.\n",
        "\n",
        "✅ Business Impact:\n",
        "Saves costs and improves customer trust by avoiding wrong predictions.\n",
        "\n",
        "3. F1-Score (Harmonic Mean of Precision & Recall)\n",
        "Balances precision and recall — critical when both false positives and false negatives are costly.\n",
        "\n",
        "✅ Business Impact:\n",
        "Achieves efficiency (correct targeting) and effectiveness (not missing real cases).\n",
        "\n",
        "4. Accuracy\n",
        "Measures the overall correctness of the model\n",
        "\n",
        "✅ Business Impact:\n",
        "Helps in communicating a broad success rate to non-technical stakeholders.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Tuned Random Forest Classifier as the final prediction model because it delivered near-perfect performance, achieving an F1-score of 1.00 for the positive class, along with 100% recall and 99% precision."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming `grid_search_rf.best_estimator_` is the best-performing model\n",
        "final_model = grid_search_rf.best_estimator_\n",
        "\n",
        "# Save the model to a .joblib file\n",
        "joblib.dump(final_model, 'tuned_random_forest_model.joblib')\n",
        "\n",
        "print(\"Model saved successfully as 'tuned_random_forest_model.joblib'\")"
      ],
      "metadata": {
        "id": "i-2b_jxnbiqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully addressed the challenge of automatically classifying Netflix content as either a movie or a TV show for the . The methodology involved a robust data science pipeline, beginning with extensive data cleaning, followed by feature engineering using techniques like TF-IDF vectorization for text data and one-hot encoding for categorical features. The analysis confirmed a significant class imbalance between movies and TV shows, which was effectively addressed by applying the SMOTE over-sampling technique to prevent model bias.\n",
        "\n",
        "Three machine learning models—Logistic Regression, Random Forest, and Gradient Boosting—were implemented, with careful attention to cross-validation and hyperparameter tuning to optimize performance. The Tuned Random Forest Classifier emerged as the superior model, achieving near-perfect performance with an F1-score of 1.00 for the minority class (TV shows). The model's high precision and recall scores indicate that it is both highly accurate and reliable in identifying all relevant content types without generating false positives.\n",
        "\n",
        "The successful implementation of this model has significant business implications. It can be integrated into the platform to automate content categorization, thereby improving the accuracy of search functions and content filters, which directly enhances user experience and engagement. Furthermore, the feature importance analysis provides valuable insights into the key attributes that differentiate content types, a finding that can inform future content acquisition strategies. This project provides a robust and deployable solution that adds tangible value to the business by leveraging machine learning to solve a core content management problem."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53c2b2b7"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86dd214f"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88f2821d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ac42010"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}